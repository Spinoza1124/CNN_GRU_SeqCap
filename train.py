import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pickle
import argparse
import os
import json
import time
from datetime import datetime
from tqdm import tqdm
import sys
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œå°†è·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
from collections import defaultdict

from model.CNN_GRU_SeqCap import CNN_SeqCap

class DynamicLearningRateScheduler:
    """åŸºäºè®­ç»ƒæŸå¤±çš„åŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, initial_lr=0.001, window_size=100):
        self.optimizer = optimizer
        self.initial_lr = initial_lr
        self.window_size = window_size
        self.loss_history = []
        self.lr_schedule = [0.001, 0.0005, 0.0002, 0.0001]
        self.current_lr_index = 0
        self.last_avg_loss = float('inf')
        
    def step(self, loss):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self.loss_history.append(loss)
        
        # åªæœ‰å½“æŸå¤±å†å²è¾¾åˆ°çª—å£å¤§å°æ—¶æ‰å¼€å§‹è°ƒæ•´
        if len(self.loss_history) >= self.window_size:
            # è®¡ç®—æœ€è¿‘100æ­¥çš„å¹³å‡æŸå¤±
            recent_losses = self.loss_history[-self.window_size:]
            current_avg_loss = sum(recent_losses) / len(recent_losses)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é™ä½å­¦ä¹ ç‡ï¼ˆæŸå¤±ä¸‹é™10å€ï¼‰
            if self.last_avg_loss / current_avg_loss >= 10.0 and self.current_lr_index < len(self.lr_schedule) - 1:
                self.current_lr_index += 1
                new_lr = self.lr_schedule[self.current_lr_index]
                
                # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = new_lr
                
                print(f"\nğŸ”„ å­¦ä¹ ç‡è°ƒæ•´: {self.get_current_lr():.6f} -> {new_lr:.6f}")
                print(f"   è§¦å‘æ¡ä»¶: å¹³å‡æŸå¤±ä» {self.last_avg_loss:.6f} é™è‡³ {current_avg_loss:.6f}")
                
                self.last_avg_loss = current_avg_loss
            elif len(self.loss_history) == self.window_size:
                # ç¬¬ä¸€æ¬¡è®¡ç®—å¹³å‡æŸå¤±
                self.last_avg_loss = current_avg_loss
    
    def get_current_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizer.param_groups[0]['lr']
    
    def get_lr_info(self):
        """è·å–å­¦ä¹ ç‡è°ƒåº¦ä¿¡æ¯"""
        return {
            'current_lr': self.get_current_lr(),
            'lr_index': self.current_lr_index,
            'loss_history_length': len(self.loss_history),
            'last_avg_loss': self.last_avg_loss
        }

class IEMOCAPDataset(Dataset):
    """IEMOCAPæ•°æ®é›†åŠ è½½å™¨"""
    def __init__(self, data_dict, transform=None):
        """
        Args:
            data_dict: åŒ…å«seg_spec, seg_labelç­‰çš„å­—å…¸
            transform: æ•°æ®å˜æ¢å‡½æ•°
        """
        self.seg_spec = data_dict['seg_spec']  # (N, 1, 200, 300)
        self.seg_label = data_dict['seg_label']  # (N,)
        self.transform = transform
        
        # æ•°æ®é¢„å¤„ç†ï¼šè°ƒæ•´ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥ [batch, 1, freq, time]
        # åŸå§‹æ•°æ®æ˜¯ (N, 1, 200, 300)ï¼Œéœ€è¦è½¬æ¢ä¸º (N, 1, 200, 300)
        # æ¨¡å‹æœŸæœ›è¾“å…¥æ˜¯ [batch, 1, freq_bins, time_steps]
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.seg_spec.shape}")
        
    def __len__(self):
        return len(self.seg_spec)
    
    def __getitem__(self, idx):
        spec = self.seg_spec[idx]  # (1, 200, 300)
        label = self.seg_label[idx]
        
        # è½¬æ¢ä¸ºtorch tensor
        spec = torch.FloatTensor(spec)
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            spec = self.transform(spec)
            
        return spec, label

def load_data_by_session(data_path):
    """æŒ‰SessionåŠ è½½IEMOCAPæ•°æ®ï¼Œä¿æŒSessionç»“æ„"""
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {data_path}")
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    session_data = {}
    total_samples = 0
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶å¤„ç†
    if isinstance(list(data.values())[0], dict) and 'features' in list(list(data.values())[0].values())[0]:
        # æµ‹è¯•æ•°æ®æ ¼å¼ï¼šSession1 -> {Session1_M: {features: [], labels: []}, Session1_F: {...}}
        for session_key in sorted(data.keys()):
            session_info = data[session_key]
            
            # åˆå¹¶è¯¥Sessionä¸‹æ‰€æœ‰è¯´è¯äººçš„æ•°æ®
            all_features = []
            all_labels = []
            
            for speaker_key in session_info.keys():
                speaker_data = session_info[speaker_key]
                all_features.extend(speaker_data['features'])
                all_labels.extend(speaker_data['labels'])
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            session_data[session_key] = {
                'seg_spec': np.array(all_features),
                'seg_label': np.array(all_labels)
            }
            
            num_samples = len(all_features)
            total_samples += num_samples
            print(f"Session {session_key}: {num_samples} ä¸ªæ ·æœ¬")
    else:
        # çœŸå®IEMOCAPæ•°æ®æ ¼å¼ï¼š1F, 1M, 2F, 2M, ...
        # éœ€è¦å°†åŒä¸€Sessionçš„ä¸åŒè¯´è¯äººæ•°æ®åˆå¹¶
        session_groups = {}
        
        for session_key in sorted(data.keys()):
            # æå–Sessionç¼–å·ï¼ˆå¦‚"1F" -> "Session1", "2M" -> "Session2"ï¼‰
            session_num = session_key[0]  # å–ç¬¬ä¸€ä¸ªå­—ç¬¦ä½œä¸ºsessionç¼–å·
            session_id = f"Session{session_num}"
            
            if session_id not in session_groups:
                session_groups[session_id] = []
            session_groups[session_id].append(session_key)
            
            num_samples = data[session_key]['seg_spec'].shape[0]
            total_samples += num_samples
            print(f"{session_key}: {num_samples} ä¸ªæ ·æœ¬")
        
        # åˆå¹¶åŒä¸€Sessionçš„æ•°æ®
        for session_id, session_keys in session_groups.items():
            all_features = []
            all_labels = []
            
            for session_key in session_keys:
                session_info = data[session_key]
                all_features.append(session_info['seg_spec'])
                all_labels.append(session_info['seg_label'])
            
            # åˆå¹¶æ•°æ®
            session_data[session_id] = {
                'seg_spec': np.concatenate(all_features, axis=0),
                'seg_label': np.concatenate(all_labels, axis=0)
            }
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"åˆå¹¶åçš„Sessionæ•°: {len(session_data)}")
    for session_id in sorted(session_data.keys()):
        print(f"{session_id}: {session_data[session_id]['seg_spec'].shape[0]} ä¸ªæ ·æœ¬")
    
    return session_data

def create_five_fold_splits(session_data):
    """åˆ›å»ºäº”æŠ˜äº¤å‰éªŒè¯çš„æ•°æ®åˆ’åˆ†
    
    Args:
        session_data: æŒ‰Sessionç»„ç»‡çš„æ•°æ®å­—å…¸
        
    Returns:
        List of 5 folds, each containing (train_data, val_data, test_data)
    """
    sessions = list(session_data.keys())
    if len(sessions) != 5:
        raise ValueError(f"IEMOCAPåº”è¯¥æœ‰5ä¸ªSessionï¼Œä½†æ‰¾åˆ°äº†{len(sessions)}ä¸ª")
    
    folds = []
    
    for i, test_session in enumerate(sessions):
        print(f"\n=== åˆ›å»ºç¬¬{i+1}æŠ˜äº¤å‰éªŒè¯ ===")
        print(f"æµ‹è¯•Session: {test_session}")
        
        # è®­ç»ƒé›†ï¼šå…¶ä»–4ä¸ªSessionçš„æ•°æ®
        train_sessions = [s for s in sessions if s != test_session]
        print(f"è®­ç»ƒSessions: {train_sessions}")
        
        # åˆå¹¶è®­ç»ƒæ•°æ®
        train_specs = []
        train_labels = []
        for session in train_sessions:
            train_specs.append(session_data[session]['seg_spec'])
            train_labels.append(session_data[session]['seg_label'])
        
        train_data = {
            'seg_spec': np.concatenate(train_specs, axis=0),
            'seg_label': np.concatenate(train_labels, axis=0)
        }
        
        # æµ‹è¯•Sessionçš„æ•°æ®éœ€è¦åˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
        test_session_spec = session_data[test_session]['seg_spec']
        test_session_label = session_data[test_session]['seg_label']
        
        # æŒ‰è¯´è¯äººåˆ’åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆå‡è®¾å‰ä¸€åŠæ˜¯ä¸€ä¸ªè¯´è¯äººï¼Œåä¸€åŠæ˜¯å¦ä¸€ä¸ªè¯´è¯äººï¼‰
        n_samples = len(test_session_spec)
        mid_point = n_samples // 2
        
        val_data = {
            'seg_spec': test_session_spec[:mid_point],
            'seg_label': test_session_label[:mid_point]
        }
        
        test_data = {
            'seg_spec': test_session_spec[mid_point:],
            'seg_label': test_session_label[mid_point:]
        }
        
        print(f"è®­ç»ƒé›†: {len(train_data['seg_spec'])} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_data['seg_spec'])} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_data['seg_spec'])} æ ·æœ¬")
        
        folds.append((train_data, val_data, test_data))
    
    return folds

def normalize_data(train_data, val_data, test_data):
    """å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ˆé›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–ï¼‰
    
    Args:
        train_data, val_data, test_data: æ•°æ®å­—å…¸
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ•°æ®å’Œæ ‡å‡†åŒ–å‚æ•°
    """
    # è®¡ç®—è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®
    train_spec = train_data['seg_spec']
    mean = np.mean(train_spec, axis=(0, 2, 3), keepdims=True)  # ä¿æŒç»´åº¦ç”¨äºå¹¿æ’­
    std = np.std(train_spec, axis=(0, 2, 3), keepdims=True)
    
    # é¿å…é™¤é›¶
    std = np.where(std == 0, 1, std)
    
    print(f"æ•°æ®æ ‡å‡†åŒ–å‚æ•°: mean={mean.flatten()}, std={std.flatten()}")
    
    # æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®é›†
    normalized_train = {
        'seg_spec': (train_data['seg_spec'] - mean) / std,
        'seg_label': train_data['seg_label'].copy()
    }
    
    normalized_val = {
        'seg_spec': (val_data['seg_spec'] - mean) / std,
        'seg_label': val_data['seg_label'].copy()
    }
    
    normalized_test = {
        'seg_spec': (test_data['seg_spec'] - mean) / std,
        'seg_label': test_data['seg_label'].copy()
    }
    
    normalization_params = {'mean': mean, 'std': std}
    
    return normalized_train, normalized_val, normalized_test, normalization_params

def split_data(data_dict, train_ratio=0.8, val_ratio=0.1, random_seed=42):
    """åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†"""
    np.random.seed(random_seed)
    
    total_samples = len(data_dict['seg_spec'])
    indices = np.random.permutation(total_samples)
    
    train_end = int(total_samples * train_ratio)
    val_end = int(total_samples * (train_ratio + val_ratio))
    
    train_indices = indices[:train_end]
    val_indices = indices[train_end:val_end]
    test_indices = indices[val_end:]
    
    def create_subset(indices):
        return {
            'seg_spec': data_dict['seg_spec'][indices],
            'seg_label': data_dict['seg_label'][indices]
        }
    
    train_data = create_subset(train_indices)
    val_data = create_subset(val_indices)
    test_data = create_subset(test_indices)
    
    print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†={len(train_indices)}, éªŒè¯é›†={len(val_indices)}, æµ‹è¯•é›†={len(test_indices)}")
    
    return train_data, val_data, test_data

def calculate_metrics(y_true, y_pred, num_classes=4):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = np.zeros((num_classes, num_classes), dtype=int)
    for i in range(len(y_true)):
        cm[y_true[i], y_pred[i]] += 1
    
    # æœªåŠ æƒå‡†ç¡®ç‡ (UA) - æ¯ä¸ªç±»åˆ«å‡†ç¡®ç‡çš„å¹³å‡
    class_accuracies = []
    for i in range(num_classes):
        if cm[i].sum() > 0:
            class_acc = cm[i, i] / cm[i].sum()
            class_accuracies.append(class_acc)
        else:
            class_accuracies.append(0.0)
    
    ua = np.mean(class_accuracies)
    
    # åŠ æƒå‡†ç¡®ç‡ (WA) - æ€»ä½“å‡†ç¡®ç‡
    wa = np.sum(y_true == y_pred) / len(y_true)
    
    return {
        'UA': ua,
        'WA': wa,
        'class_accuracies': class_accuracies,
        'confusion_matrix': cm.tolist()  # è½¬æ¢ä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
    }

def train_epoch(model, dataloader, criterion, optimizer, device, lr_scheduler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(dataloader, desc="è®­ç»ƒä¸­", file=sys.stdout)
    
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if lr_scheduler is not None:
            lr_scheduler.step(loss.item())
        
        total_loss += loss.item()
        
        # é¢„æµ‹ç»“æœ
        pred = output.argmax(dim=1)
        all_preds.extend(pred.cpu().numpy())
        all_labels.extend(target.cpu().numpy())
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        current_avg_loss = total_loss / (batch_idx + 1)
        current_lr = lr_scheduler.get_current_lr() if lr_scheduler else optimizer.param_groups[0]['lr']
        pbar.set_postfix({
            'Loss': f'{loss.item():.6f}',
            'Avg_Loss': f'{current_avg_loss:.6f}',
            'LR': f'{current_lr:.6f}'
        })
    
    avg_loss = total_loss / len(dataloader)
    metrics = calculate_metrics(all_labels, all_preds)
    
    return avg_loss, metrics

def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(dataloader, desc="éªŒè¯ä¸­", file=sys.stdout)
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            
            output = model(data)
            loss = criterion(output, target)
            
            total_loss += loss.item()
            
            pred = output.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            current_avg_loss = total_loss / (batch_idx + 1)
            pbar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Avg_Loss': f'{current_avg_loss:.6f}'
            })
    
    avg_loss = total_loss / len(dataloader)
    metrics = calculate_metrics(all_labels, all_preds)
    
    return avg_loss, metrics

def save_experiment_log(log_data, save_dir):
    """ä¿å­˜å®éªŒæ—¥å¿—"""
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜JSONæ ¼å¼çš„æ—¥å¿—
    log_file = os.path.join(save_dir, 'experiment_log.json')
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, indent=2, ensure_ascii=False)
    
    print(f"å®éªŒæ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

def plot_training_curves(train_losses, val_losses, train_metrics, val_metrics, save_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    if not MATPLOTLIB_AVAILABLE:
        print("è·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶ (matplotlibæœªå®‰è£…)")
        return
        
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
    plt.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±')
    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # UAæ›²çº¿
    plt.subplot(1, 3, 2)
    train_ua = [m['UA'] for m in train_metrics]
    val_ua = [m['UA'] for m in val_metrics]
    plt.plot(epochs, train_ua, 'b-', label='è®­ç»ƒUA')
    plt.plot(epochs, val_ua, 'r-', label='éªŒè¯UA')
    plt.title('æœªåŠ æƒå‡†ç¡®ç‡ (UA)')
    plt.xlabel('Epoch')
    plt.ylabel('UA')
    plt.legend()
    plt.grid(True)
    
    # WAæ›²çº¿
    plt.subplot(1, 3, 3)
    train_wa = [m['WA'] for m in train_metrics]
    val_wa = [m['WA'] for m in val_metrics]
    plt.plot(epochs, train_wa, 'b-', label='è®­ç»ƒWA')
    plt.plot(epochs, val_wa, 'r-', label='éªŒè¯WA')
    plt.title('åŠ æƒå‡†ç¡®ç‡ (WA)')
    plt.xlabel('Epoch')
    plt.ylabel('WA')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'training_curves.png')}")

def train_single_fold(fold_idx, train_data, val_data, test_data, config, experiment_dir):
    """è®­ç»ƒå•ä¸ªæŠ˜"""
    print(f"\n{'='*80}")
    print(f"ğŸ”„ å¼€å§‹ç¬¬ {fold_idx + 1} æŠ˜è®­ç»ƒ")
    print(f"{'='*80}")
    
    # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…CUDAå…¼å®¹æ€§é—®é¢˜
    device = torch.device('cpu')
    
    # æ•°æ®æ ‡å‡†åŒ–
    train_data, val_data, test_data, normalization_params = normalize_data(train_data, val_data, test_data)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = IEMOCAPDataset(train_data)
    val_dataset = IEMOCAPDataset(val_data)
    test_dataset = IEMOCAPDataset(test_data)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    model = CNN_SeqCap(num_classes=config['num_classes']).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼ˆæŒ‰ç…§è§„èŒƒé…ç½®ï¼‰
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], 
                          betas=(0.9, 0.999), eps=1e-8)
    
    # åˆ›å»ºåŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = DynamicLearningRateScheduler(optimizer, initial_lr=config['learning_rate'])
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    val_losses = []
    train_metrics = []
    val_metrics = []
    best_val_wa = 0.0  # ä½¿ç”¨WAä½œä¸ºæ¨¡å‹é€‰æ‹©æ ‡å‡†
    best_model_state = None
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (å…± {config['epochs']} ä¸ªepoch)")
    start_time = time.time()
    
    # åˆ›å»ºepochçº§åˆ«çš„è¿›åº¦æ¡
    epoch_pbar = tqdm(range(config['epochs']), desc=f"Fold {fold_idx + 1}", position=0, leave=True)
    
    for epoch in epoch_pbar:
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_metric = train_epoch(model, train_loader, criterion, optimizer, device, lr_scheduler)
        train_losses.append(train_loss)
        train_metrics.append(train_metric)
        
        # éªŒè¯
        val_loss, val_metric = evaluate(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_metrics.append(val_metric)
        
        # è®¡ç®—epochè€—æ—¶
        epoch_time = time.time() - epoch_start_time
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†WAï¼‰
        if val_metric['WA'] > best_val_wa:
            best_val_wa = val_metric['WA']
            best_model_state = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_ua': val_metric['UA'],
                'val_wa': val_metric['WA'],
                'fold': fold_idx
            }
        
        # æ›´æ–°è¿›åº¦æ¡
        lr_info = lr_scheduler.get_lr_info()
        epoch_pbar.set_postfix({
            'T_Loss': f'{train_loss:.4f}',
            'V_WA': f'{val_metric["WA"]:.4f}',
            'Best_WA': f'{best_val_wa:.4f}',
            'LR': f'{lr_info["current_lr"]:.6f}'
        })
        
        # æ¯5ä¸ªepochæ‰“å°è¯¦ç»†ä¿¡æ¯
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"\nğŸ“Š Epoch {epoch+1}/{config['epochs']} ç»“æœ:")
            print(f"   è®­ç»ƒ: Loss={train_loss:.6f}, UA={train_metric['UA']:.4f}, WA={train_metric['WA']:.4f}")
            print(f"   éªŒè¯: Loss={val_loss:.6f}, UA={val_metric['UA']:.4f}, WA={val_metric['WA']:.4f}")
            print(f"   å­¦ä¹ ç‡: {lr_info['current_lr']:.6f}, è€—æ—¶: {epoch_time:.2f}s")
    
    epoch_pbar.close()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    if best_model_state is not None:
        model.load_state_dict(best_model_state['model_state_dict'])
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_loss, test_metric = evaluate(model, test_loader, criterion, device)
    
    training_time = time.time() - start_time
    
    print(f"\nâœ… ç¬¬ {fold_idx + 1} æŠ˜è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    print(f"   æœ€ä½³éªŒè¯WA: {best_val_wa:.4f}")
    print(f"   æµ‹è¯•ç»“æœ: UA={test_metric['UA']:.4f}, WA={test_metric['WA']:.4f}")
    
    # ä¿å­˜æŠ˜çš„ç»“æœ
    fold_results = {
        'fold_idx': fold_idx,
        'training_time': training_time,
        'best_val_wa': best_val_wa,
        'test_results': {
            'loss': test_loss,
            'UA': test_metric['UA'],
            'WA': test_metric['WA'],
            'class_accuracies': test_metric['class_accuracies']
        },
        'training_history': {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_ua': [m['UA'] for m in train_metrics],
            'val_ua': [m['UA'] for m in val_metrics],
            'train_wa': [m['WA'] for m in train_metrics],
            'val_wa': [m['WA'] for m in val_metrics]
        }
    }
    
    # ä¿å­˜æŠ˜çš„æ¨¡å‹å’Œç»“æœ
    fold_dir = os.path.join(experiment_dir, f'fold_{fold_idx + 1}')
    os.makedirs(fold_dir, exist_ok=True)
    
    if best_model_state is not None:
        torch.save(best_model_state, os.path.join(fold_dir, 'best_model.pth'))
    
    with open(os.path.join(fold_dir, 'results.json'), 'w', encoding='utf-8') as f:
        json.dump(fold_results, f, indent=2, ensure_ascii=False)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, train_metrics, val_metrics, fold_dir)
    
    return fold_results

def main():
    parser = argparse.ArgumentParser(description='CNN-GRU-SeqCapäº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒè„šæœ¬')
    parser.add_argument('--data_path', type=str, default='data/IEMOCAP_multi.pkl',
                        help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='æ‰¹é‡å¤§å°ï¼ˆå›ºå®šä¸º16ï¼‰')
    parser.add_argument('--epochs', type=int, default=20,
                        help='è®­ç»ƒè½®æ•°ï¼ˆå›ºå®šä¸º20ï¼‰')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='åˆå§‹å­¦ä¹ ç‡ï¼ˆå›ºå®šä¸º0.001ï¼‰')
    parser.add_argument('--num_classes', type=int, default=4,
                        help='åˆ†ç±»æ•°é‡')
    parser.add_argument('--save_dir', type=str, default='experiments',
                        help='å®éªŒç»“æœä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…CUDAå…¼å®¹æ€§é—®é¢˜
    device = torch.device('cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print("âš ï¸  æ³¨æ„: æ£€æµ‹åˆ°CUDAä½†ç”±äºå…¼å®¹æ€§é—®é¢˜ä½¿ç”¨CPUè¿è¡Œ")
    
    # åˆ›å»ºå®éªŒç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = os.path.join(args.save_dir, f"five_fold_cv_{timestamp}")
    os.makedirs(experiment_dir, exist_ok=True)
    
    # ä¿å­˜è¶…å‚æ•°é…ç½®
    config = {
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'learning_rate': args.lr,
        'num_classes': args.num_classes,
        'device': str(device),
        'data_path': args.data_path,
        'timestamp': timestamp,
        'experiment_type': 'five_fold_cross_validation',
        'optimizer': 'Adam(Î²1=0.9, Î²2=0.999, Îµ=1e-8)',
        'initialization': 'Xavier',
        'lr_schedule': 'Dynamic(window=100, ratios=[0.001, 0.0005, 0.0002, 0.0001])',
        'model_selection': 'Best validation WA'
    }
    
    print("\nğŸ“‹ äº”æŠ˜äº¤å‰éªŒè¯é…ç½®")
    print("="*50)
    for key, value in config.items():
        print(f"{key}: {value}")
    
    # åŠ è½½æ•°æ®å¹¶åˆ›å»ºäº”æŠ˜åˆ’åˆ†
    print("\nğŸ“‚ æ•°æ®åŠ è½½ä¸åˆ’åˆ†")
    print("="*50)
    session_data = load_data_by_session(args.data_path)
    fold_splits = create_five_fold_splits(session_data)
    
    print(f"âœ… æˆåŠŸåˆ›å»º {len(fold_splits)} æŠ˜æ•°æ®åˆ’åˆ†")
    for i, (train_sessions, val_session, test_session) in enumerate(fold_splits):
        print(f"   Fold {i+1}: è®­ç»ƒ={train_sessions}, éªŒè¯={val_session}, æµ‹è¯•={test_session}")
    
    # æ‰§è¡Œäº”æŠ˜äº¤å‰éªŒè¯
    all_fold_results = []
    total_start_time = time.time()
    
    print("\nğŸš€ å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯")
    print("="*80)
    
    for fold_idx, (train_data, val_data, test_data) in enumerate(fold_splits):
        # æ•°æ®å·²ç»åœ¨create_five_fold_splitsä¸­å‡†å¤‡å¥½äº†
        
        # è®­ç»ƒå½“å‰æŠ˜
        fold_result = train_single_fold(fold_idx, train_data, val_data, test_data, config, experiment_dir)
        all_fold_results.append(fold_result)
    
    total_time = time.time() - total_start_time
    
    # è®¡ç®—äº”æŠ˜äº¤å‰éªŒè¯çš„æ€»ä½“ç»“æœ
    print("\nğŸ“Š äº”æŠ˜äº¤å‰éªŒè¯æ€»ä½“ç»“æœ")
    print("="*80)
    
    test_uas = [result['test_results']['UA'] for result in all_fold_results]
    test_was = [result['test_results']['WA'] for result in all_fold_results]
    val_was = [result['best_val_wa'] for result in all_fold_results]
    
    mean_test_ua = np.mean(test_uas)
    std_test_ua = np.std(test_uas)
    mean_test_wa = np.mean(test_was)
    std_test_wa = np.std(test_was)
    mean_val_wa = np.mean(val_was)
    std_val_wa = np.std(val_was)
    
    print(f"ğŸ¯ æµ‹è¯•é›† UA: {mean_test_ua:.4f} Â± {std_test_ua:.4f}")
    print(f"ğŸ¯ æµ‹è¯•é›† WA: {mean_test_wa:.4f} Â± {std_test_wa:.4f}")
    print(f"ğŸ¯ éªŒè¯é›† WA: {mean_val_wa:.4f} Â± {std_val_wa:.4f}")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}ç§’")
    
    print("\nğŸ“‹ å„æŠ˜è¯¦ç»†ç»“æœ:")
    for i, result in enumerate(all_fold_results):
        print(f"   Fold {i+1}: æµ‹è¯•UA={result['test_results']['UA']:.4f}, "
              f"æµ‹è¯•WA={result['test_results']['WA']:.4f}, "
              f"éªŒè¯WA={result['best_val_wa']:.4f}")
    
    # ä¿å­˜æ€»ä½“å®éªŒç»“æœ
    final_results = {
        'config': config,
        'total_time': total_time,
        'cross_validation_results': {
            'mean_test_ua': mean_test_ua,
            'std_test_ua': std_test_ua,
            'mean_test_wa': mean_test_wa,
            'std_test_wa': std_test_wa,
            'mean_val_wa': mean_val_wa,
            'std_val_wa': std_val_wa,
            'individual_folds': all_fold_results
        }
    }
    
    with open(os.path.join(experiment_dir, 'final_results.json'), 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {experiment_dir}")
    print(f"ğŸ† æœ€ç»ˆç»“æœ: æµ‹è¯•UA={mean_test_ua:.4f}Â±{std_test_ua:.4f}, æµ‹è¯•WA={mean_test_wa:.4f}Â±{std_test_wa:.4f}")

if __name__ == "__main__":
    main()